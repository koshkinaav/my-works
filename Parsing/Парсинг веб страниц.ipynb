{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1794e432",
   "metadata": {},
   "source": [
    "# Задача о знакомствах. Ищем через сколько ссылок в википедии можно выйти на нужного человека. (в данном примере для Пугачевой и Трампа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3176b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab813487",
   "metadata": {},
   "source": [
    "Пишем функцию getLinks, которая возвращает список ссылок лежащщих на странице в википедии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1899fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinks(articleUrl):\n",
    "    list_uf_urls = []\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(articleUrl))\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    for link in bs.find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')):\n",
    "        if 'href' in link.attrs:\n",
    "            list_uf_urls.append(link.attrs['href'])\n",
    "    return list_uf_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a6efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### введите ссылку с которой хотите начать поиск\n",
    "\n",
    "url = '/wiki/Alla_Pugacheva'\n",
    "\n",
    "###\n",
    "html = urlopen('http://en.wikipedia.org{}'.format(url))\n",
    "bS = BeautifulSoup(html, 'html.parser')\n",
    "count = 0\n",
    "while (bS.find_all(text='Trump') == [] and count < 1000):\n",
    "    links = getLinks(url)\n",
    "    url = np.random.choice(links)\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(url))\n",
    "    bS = BeautifulSoup(html, 'html.parser')\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d80388d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b53296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/John_F._Kennedy'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9496a",
   "metadata": {},
   "source": [
    "# Функция, ищущая все внешние и веутренние ссылки на странице  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f839b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Проверяет, является ли url допустимым\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1a9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import colorama\n",
    "\n",
    "# запускаем модуль colorama\n",
    "colorama.init()\n",
    "\n",
    "GREEN = colorama.Fore.GREEN\n",
    "GRAY = colorama.Fore.LIGHTBLACK_EX # цвета \n",
    "RESET = colorama.Fore.RESET\n",
    "RED = colorama.Fore.RED\n",
    "\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "\n",
    "total_urls_visited = 0\n",
    "\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Проверка url\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "def get_all_website_links(url):\n",
    "    \"\"\"\n",
    "    Возвращает все найденные URL-адреса на `url, того же веб-сайта.\n",
    "    \"\"\"\n",
    "    # все URL-адреса `url`\n",
    "    urls = set()\n",
    "    # доменное имя URL без протокола\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            # пустой тег href\n",
    "            continue\n",
    "        # присоединяемся к URL, если он относительный (не абсолютная ссылка)\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        # удалить параметры URL GET, фрагменты URL и т. д.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            # неверный URL\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            # уже в наборе\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            # внешняя ссылка\n",
    "            if href not in external_urls:\n",
    "                print(f\"{RED}[!] Внешняя ссылка: {href}{RESET}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "        print(f\"{GREEN}[*] Внутреннея ссылка: {href}{RESET}\")\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return internal_urls, external_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1d44f",
   "metadata": {},
   "source": [
    "URLPARSE - именованный кортеж, делящий ссылку на ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "054c8e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResult(scheme='https', netloc='sites.google.com', path='/view/qm1-hse/', params='', query='', fragment='')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlparse('https://sites.google.com/view/qm1-hse/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ac13c3",
   "metadata": {},
   "source": [
    "# Сохраняем фото Дональда Трампа (парсинг медиа с интернета)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "photos = []\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Donald_Trump')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "imageLocation = bs.find_all('img')\n",
    "for ek in imageLocation:\n",
    "    photos.append(ek['src'])\n",
    "\n",
    "urlretrieve('https:'+ photos[1], 'logo.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2e6dd0",
   "metadata": {},
   "source": [
    "# Скачиваем таблицу из википедии (парсинг таблиц)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8488d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Euler%27s_totient_function')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "table = bs.find_all('table', {'class': 'wikitable'})[0]\n",
    "rows = table.find_all('tr')\n",
    "csvFile = open('editors.csv', 'wt+')\n",
    "writer = csv.writer(csvFile)\n",
    "csvRow = []\n",
    "try:\n",
    "    for row in rows:\n",
    "        for cell in row.find_all(['td', 'th']):\n",
    "            csvRow.append(cell.get_text())\n",
    "            writer.writerow(csvRow)\n",
    "finally:\n",
    "    width = len(row.find_all(['td', 'th']))\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e7fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a0af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('file.txt', 'w') as f:\n",
    "    for i in range (1, len(csvRow)):\n",
    "        if i % width == 0:\n",
    "            f.write(csvRow[i])\n",
    "            f.write('\\n')\n",
    "   \n",
    "        elif(i % width == (width-1) ):\n",
    "            f.write(csvRow[i].replace('\\n', ''))\n",
    "        else:\n",
    "            f.write(csvRow[i].strip() + '±')\n",
    "            \n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b68860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('file.txt', sep='±')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e8b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f5a784",
   "metadata": {},
   "source": [
    "# Данная программа ищет текст (слово) на всех ссылках с данного сайта и возвращет ссылки, где текст был найден"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd465d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.littre.org/definition/dire\n",
      "https://www.littre.org/definition/race\n",
      "https://www.littre.org/annexes/preface\n",
      "https://www.littre.org\n",
      "https://www.littre.org/annexes/causerie\n",
      "http://creativecommons.org/licenses/by-sa/3.0/deed.fr\n",
      "https://www.littre.org/\n",
      "https://www.littre.org/definition/garce\n",
      "https://www.littre.org/etymologie\n",
      "https://www.littre.org/definition/%C3%A9viter\n",
      "https://www.littre.org/definition/libert%C3%A9\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Проверяет, является ли url допустимым\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, name, url, tag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.tag = tag\n",
    "        self.bodyTag = bodyTag\n",
    "\n",
    "\n",
    "class Crawler(Website):\n",
    "    internal_urls = set()\n",
    "    external_urls = set()\n",
    "    objects = set()\n",
    "\n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def get_all_website_links(self, url, internal_urls=internal_urls):\n",
    "        urls = set()\n",
    "        domain_name = urlparse(url).netloc\n",
    "        soup = self.getPage(url)\n",
    "        for a_tag in soup.findAll(\"a\"):\n",
    "            href = a_tag.attrs.get(\"href\")\n",
    "            if href == \"\" or href is None:\n",
    "                continue\n",
    "            href = urljoin(url, href)\n",
    "            parsed_href = urlparse(href)\n",
    "            href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "            if not is_valid(href):\n",
    "                continue\n",
    "            if href in internal_urls:\n",
    "                continue\n",
    "            urls.add(href)\n",
    "            internal_urls.add(href)\n",
    "        internal_urls.add(url)\n",
    "        return internal_urls\n",
    "\n",
    "    def One_link(self, url, objects=objects):\n",
    "\n",
    "        bs = self.getPage(url)\n",
    "        if bs is not None:\n",
    "            bs = bs.findAll(self.tag)\n",
    "            for el in bs:\n",
    "                if self.bodyTag in el.get_text():\n",
    "                    objects.add(url)\n",
    "        return objects\n",
    "\n",
    "    def crawler(self, objects=objects):\n",
    "\n",
    "        internal_links = self.get_all_website_links(self.url)\n",
    "        for link in internal_links:\n",
    "            self.One_link(link)\n",
    "        return objects\n",
    "\n",
    "\n",
    "site = Crawler('Times', 'https://www.littre.org', 'p', 'peut')\n",
    "for el in site.crawler():\n",
    "    print(el)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
